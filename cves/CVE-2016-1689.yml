---
CVE: CVE-2016-1689
CWE: 122
announced: 2016-06-05 19:59:18.710000000 -04:00
description_instructions: |
  You can get an initial description from the CVE entry on cve.mitre.org. These
  descriptions are a fine start, but they can be kind of jargony.

  Rewrite this description in your own words. Make it interesting and easy to
  read to anyone with some programming experience. We can always pull up the NVD
  description later to get more technical.

  Try to still be specific in your description, but remove Chromium-specific
  stuff. Remove references to versions, specific filenames, and other jargon
  that outsiders to Chromium would not understand. Technology like "regular
  expressions" is fine, and security phrases like "invalid write" are fine to
  keep too.
description: |
  A Heap Buffer Overflow vulnerability was found in Google Chrome's implementation of
  Media Capture from DOM elements. Media Capture from DOM elements is essentially just
  the more information about the users actions on a video, image, etc. which is streamed
  back to the application and can then be used for various things. When Google Chrome
  added this feature they had a bug where if the HTML media element's given size parameters
  were wrong it wouldn't check them, and would just attempt to use them, resulting in the
  Heap Buffer Overflow.
bounty:
  date: 2016-05-25 15:45:00.000000000 -04:00
  amount: 1000.0
  references:
  - http://chromereleases.googleblog.com/2016/05/stable-channel-update_25.html
reviews:
- 1962993002
- 1918073003
bugs:
- 606185
repo: 
fixes_vcc_instructions: |
  Please put the commit hash in "commit" below (see my example in
  CVE-2011-3092.yml). Fixes and VCCs follow the same format.
fixes:
- :commit: e0dd9f840b3a21cd12bd3d83f5ca63302549dd21
  :note: 
vccs:
- :commit: aed06ff043b4d93701f2fd0c3c79f5bfd04395bb
  :note: |
    This commit was attempting to add functionality to generate media streams from
    a canvas.
upvotes_instructions: |
  For the first round, ignore this upvotes number.

  For the second round of reviewing, you will be giving a certain amount of
  upvotes to each vulnerability you see. Your peers will tell you how
  interesting they think this vulnerability is, and you'll add that to the
  upvotes score on your branch.
upvotes: 0
unit_tested:
  question: |
    Were automated unit tests involved in this vulnerability?
    Was the original code unit tested, or not unit tested? Did the fix involve
    improving the automated tests?

    For the "code" answer below, look not only at the fix but the surrounding
    code near the fix and determine if and was there were unit tests involved
    for this module.

    For the "fix" answer below, check if the fix for the vulnerability involves
    adding or improving an automated test to ensure this doesn't happen again.
  answer: |
    The fix code clearly contained unit tests, and the fix included updating an
    existing test case to better test what happens when giving the wrong size
    in the canvas, while also adding more unit tests.
  code: true
  fix: true
discovered:
  question: |
    How was this vulnerability discovered?

    Go to the bug report and read the conversation to find out how this was
    originally found. Answer in longform below in "answer", fill in the date in
    YYYY-MM-DD, and then determine if the vulnerability was found by a Google
    employee (you can tell from their email address). If it's clear that the
    vulenrability was discovered by a contest, fill in the name there.

    The "automated" flag can be true, false, or nil.
    The "google" flag can be true, false, or nil.

    If there is no evidence as to how this vulnerability was found, then you may
    leave this part blank.
  answer: |
    A google employee found this bug, and reported it. Then a testcase was added
    to ClusterFuzz, which then ran and tested the vulernability further.
  date: 2016-04-24
  automated: false
  google: true
  contest: 
subsystem:
  question: |
    What subsystems was the mistake in?

    Look at the path of the source code files code that were fixed to get
    directory names. Look at comments in the code. Look at the bug reports how
    the bug report was tagged.
  answer: |
    Based on the directory path.
  name: Renderer Media
interesting_commits:
  question: |
    Are there any interesting commits between your VCC(s) and fix(es)?

    Write a brief (under 100 words) description of why you think this commit was
    interesting in light of the lessons learned from this vulnerability. Any
    emerging themes?
  commits:
  - commit: 67e2c0a015fb2d2d2584a44b96da1620d2678bf5
    note: |
      Huge code migration to a new smart pointer which disposes of the object
      when that pointer is no longer in scope (std::unique_ptr).
  - commit: 
    note: 
major_events:
  question: |
    Please record any major events you found in the history of this
    vulnerability. Was the code rewritten at some point? Was a nearby subsystem
    changed? Did the team change?

    The event doesn't need to be directly related to this vulnerability, rather,
    we want to capture what the development team was dealing with at the time.
  answer: |
    Media Capture was added to HTML media and canvas elements. For reference
    `https://www.w3.org/TR/mediacapture-fromelement/`. In the comments of the code
    review for the VCC they were discussing about how FireFox already had support for
    this new feature. There seemed to be a sense of urgence to get support for this
    feature implemented.
  events:
  - name: Media Capture from DOM elements
    date: 2017-02-07
  - name: 
    date: 
lessons:
  question: |
    Are there any common lessons we have learned from class that apply to this
    vulnerability? In other words, could this vulnerability serve as an example
    of one of those lessons?

    Leave "applies" blank or put false if you did not see that lesson (you do
    not need to put a reason). Put "true" if you feel the lesson applies and put
    a quick explanation of how it applies.

    Don't feel the need to claim that ALL of these apply, but it's pretty likely
    that one or two of them apply.

    If you think of another lesson we covered in class that applies here, feel
    free to give it a small name and add one in the same format as these.
  defense_in_depth:
    applies: 
    note: 
  least_privilege:
    applies: 
    note: 
  frameworks_are_optional:
    applies: 
    note: 
  native_wrappers:
    applies: 
    note: 
  distrust_input:
    applies: true
    note: |
      They overlooked the possibility of the input from the video element being different
      from the canvas element.
  security_by_obscurity:
    applies: 
    note: 
  serial_killer:
    applies: 
    note: 
  environment_variables:
    applies: 
    note: 
  secure_by_default:
    applies: 
    note: 
  yagni:
    applies: 
    note: 
  complex_inputs:
    applies: 
    note: 
mistakes:
  question: |
    In your opinion, after all of this research, what mistakes were made that
    led to this vulnerability? Coding mistakes? Design mistakes?
    Maintainability? Requirements? Miscommunications?

    Look at the CWE entry for this vulnerability and examine the mitigations
    they have written there. Are they doing those? Does the fix look proper?

    Use those questions to inspire your answer. Don't feel obligated to answer
    every one. Write a thoughtful entry here that those ing the software
    engineering industry would find interesting.
  answer: |
    This vulnerability seemed to stem from a lack of design around the desired feature. This
    lack of design then caused the software to be implemented to trust inputs which shouldn't
    have been trusted. It also seemed that this software was created for temporary use, which
    is questionable. It doesn't seem to make sense to create software which you know later you
    are going to redo, why not just design, and create the feature completely the first time?
